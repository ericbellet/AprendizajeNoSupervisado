# Multiplicacion Matriz por Vector usando mapreduce
# Tipo 1: el caso en que el vector v cabe dentro de la memoria RAM
MultMV_1.mr <- function( M, V) {
d <- values(from.dfs(V))
f <- function(x){return(x[3]*d[x[2],2])}
map <- function(.,m) {
i <- m[1]
m <- as.matrix(m)
valor <- apply(m,1,f)
valor <- as.data.frame(as.numeric(as.character(valor)))
return( keyval(i, valor) )
}
reduce <- function(i, xi) {
keyval(i, sum(xi))
}
calc <- mapreduce(input=M,
#output=output,
#input.format="text",
map=map,
reduce=reduce,
verbose = FALSE)
C = values( from.dfs( calc ) )
C
}
library(rhdfs)
#Lectura de datos.
df = read.csv(file = "C:/Users/Eric/Desktop/AprendizajeNoSupervisado/data/a.csv")
#Modificamos el nombre de las columnas por comodidad.
colnames(df) <- c("x","y","class")
#Analisis exploratorio del dataset.
#Podemos observar que hay 3 columnas.
head(df)
#Observamos cuantos elementos hay de cada clase.
table(df$class)
#0    1    2
#1000  999 1000
plot(datos$X, datos$Y, col = datos$class,
xlab = "X", ylab = "Y",
main = "Clustering Rectangular")
plot(df$X, df$Y, col = df$class,
xlab = "X", ylab = "Y",
main = "Clustering Rectangular")
plot(df$X, df$Y, col = df$class,
xlim = c(min(datos$X-100), max(datos$X+100)),
ylim = c(min(datos$Y-100), max(datos$Y+100)),
xlab = "X", ylab = "Y",
main = "Clustering Rectangular")
plot(df$X, df$Y, col = df$class,
xlim = c(min(df$X-100), max(df$X+100)),
ylim = c(min(df$Y-100), max(df$Y+100)),
xlab = "X", ylab = "Y",
main = "Clustering Rectangular")
#Lectura de datos.
df = read.csv(file = "C:/Users/Eric/Desktop/AprendizajeNoSupervisado/data/a.csv")
#Modificamos el nombre de las columnas por comodidad.
colnames(df) <- c("x","y","class")
#Analisis exploratorio del dataset.
#Podemos observar que hay 3 columnas.
head(df)
#Observamos cuantos elementos hay de cada clase.
table(df$class)
#0    1    2
#1000  999 1000
plot(df$X, df$Y, col = df$class,
xlim = c(min(df$X-50), max(df$X+50)),
ylim = c(min(df$Y-50), max(df$Y+50)),
xlab = "X", ylab = "Y",
main = "Clustering Rectangular")
#Lectura de datos.
df = read.csv(file = "C:/Users/Eric/Desktop/AprendizajeNoSupervisado/data/a.csv")
#Modificamos el nombre de las columnas por comodidad.
colnames(df) <- c("x","y","class")
#Analisis exploratorio del dataset.
#Podemos observar que hay 3 columnas.
head(df)
#Observamos cuantos elementos hay de cada clase.
table(df$class)
#0    1    2
#1000  999 1000
plot(df$X, df$Y, col = df$class,
xlim = c(min(df$X-30), max(df$X+30)),
ylim = c(min(df$Y-30), max(df$Y+30)),
xlab = "X", ylab = "Y",
main = "Clustering Rectangular")
#Lectura de datos.
df = read.csv(file = "C:/Users/Eric/Desktop/AprendizajeNoSupervisado/data/a.csv")
#Modificamos el nombre de las columnas por comodidad.
colnames(df) <- c("x","y","class")
#Analisis exploratorio del dataset.
#Podemos observar que hay 3 columnas.
head(df)
#Observamos cuantos elementos hay de cada clase.
table(df$class)
#0    1    2
#1000  999 1000
plot(df$X, df$Y, col = df$class,
xlim = c(min(df$X-20), max(df$X+20)),
ylim = c(min(df$Y-20), max(df$Y+20)),
xlab = "X", ylab = "Y",
main = "Clustering Rectangular")
plot(df$x, df$y, col = df$class)
plot(df$x, df$y)
modelo.k.medias = kmeans(x = df[, c("x", "y")], centers = 3)
plot(x = df$x, y = df$y, col = modelo.k.medias$cluster)
points(x = modelo.k.medias$centers[, c("x", "y")], col = 1:4, pch = 19, cex = 3)
plot(df$x, df$y), col = df$class)
plot(df$x, df$y, col = df$class)
head(df)
plot(df$x, df$y, col = df$class,
xlim = c(min(df$x-20), max(df$x+20)),
ylim = c(min(df$y-0), max(df$y+20)),
xlab = "X", ylab = "Y",
main = "Clustering Rectangular")
plot(df$x, df$y)
table(modelo.k.medias$cluster, df$class)
modelo.k.medias$cluster
df$class
modelo.k.medias$cluster
modelo.k.medias
matrizconfusion <- table(df$class,modelo.k.medias$cluster,dnn=c("Valor Real", "Prediccion"))
matrizconfusion
matrizconfusion <- table(df$class,modelo.k.medias$cluster,dnn=c("Valor Real", "Cluster"))
matrizconfusion
df = read.csv(file = "C:/Users/Eric/Desktop/AprendizajeNoSupervisado/data/a_big.csv")
View(df)
View(df)
colnames(df) <- c("x","y","class")
plot(df$x, df$y)
modelo.k.medias = kmeans(x = df[, c("x", "y")], centers = 3)
plot(x = df$x, y = df$y, col = modelo.k.medias$cluster)
points(x = modelo.k.medias$centers[, c("x", "y")], col = 1:4, pch = 19, cex = 3)
table(df$class)
matrizconfusion <- table(df$class,modelo.k.medias$cluster,dnn=c("Valor Real", "Cluster"))
matrizconfusion
df = read.csv(file = "C:/Users/Eric/Desktop/AprendizajeNoSupervisado/data/good_luck.csv")
colnames(df)[11] <- "class"
table(df$class)
plot(df)
plot(df)
modelo.k.medias = kmeans(x = df[, -c("class")], centers = 3)
head(df)
modelo.k.medias = kmeans(x = df[, c("X.0.262989", "X.0.868793", "X0.133290" ,
"X2.745286", "X.0.047937", "X1.357079", "X.0.499947",
"X.1.874985", "X.0.395397","X1.563203")], centers = 3)
table(modelo.k.medias$cluster, df$class)
modelo.k.medias = kmeans(x = df[, c("X.0.262989", "X.0.868793", "X0.133290" ,
"X2.745286", "X.0.047937", "X1.357079", "X.0.499947",
"X.1.874985", "X.0.395397","X1.563203")], centers = 2)
table(modelo.k.medias$cluster, df$class)
entrada.num = df
entrada.num$Class = NULL
entrada.num = as.matrix(entrada.num)
distancia = dist(entrada.num)
distancia
cluster = hclust(distancia, method = metodo)
metodo = "complete"
cluster = hclust(distancia, method = metodo)
plot(cluster)
nclases = 2
corte = cutree(cluster, k=nclases)
table(df$class, corte)
corte = cutree(cluster, h=15)
unique(corte)
corte = cutree(cluster, h=13)
unique(corte)
corte = cutree(cluster, h=5)
unique(corte)
corte = cutree(cluster, h=9)
unique(corte)
corte = cutree(cluster, h=11)
# Verificamos cuántos clúster tenemos
unique(corte)
corte = cutree(cluster, h=10)
unique(corte)
corte = cutree(cluster, h=9.5)
# Verificamos cuántos clúster tenemos
unique(corte)
corte = cutree(cluster, h=9.0)
# Verificamos cuántos clúster tenemos
unique(corte)
corte = cutree(cluster, h=9.2)
# Verificamos cuántos clúster tenemos
unique(corte)
corte = cutree(cluster, h=9.3)
# Verificamos cuántos clúster tenemos
unique(corte)
table(df$class, corte)
plot(df)
x <- c(2,6,67,85,7,9,4,21,78,45)
cle <- ifelse (x %% 2 == 0, 1, 2)
cle
keyval(cle,x)
x <- c(2,6,67,85,7,9,4,21,78,45)
cle <- ifelse (x %% 2 == 0, 1, 2)
keyval(cle,x)
library(rmr2)
ignore <- rmr.options(backend="local") # Opciones "local" o "hadoop"
setwd("C:/Users/Eric/Desktop/AprendizajeNoSupervisado")
source("src/funciones.R")
name = "h.csv"
library(rgl)
library('FactoMineR')
library(pROC)
#Lectura de datos.
df = read.csv(file = "C:/Users/Eric/Desktop/AprendizajeNoSupervisado/data/h.csv", header = F)
#Modificamos el nombre de las columnas por comodidad.
colnames(df) <- c("x","y","z","class")
#df <- df[ order(df$class), ]
#Coloco las clases del 1:n
df$class = as.numeric(df$class)
if (min(df$class) == 0){
df$class <- df$class + 1
}
#****************************************************************************************
#                   Analisis exploratorio del dataset
#****************************************************************************************
#Podemos observar que hay 4 columnas.
head(df)
#Matriz de dispersion
plot(df)
PCA <- PCA(df)
plot(PCA)
plot(PCA$ind$coord[,1],PCA$ind$coord[,3])
#Grafico
palette(rainbow(14))
plot3d(df$x, df$y, df$z, type = "s", size = 2, col = df$class)
#Podemos observar 11 conglomerados
length(unique(df$class))
#Existen 11 clases.
#*********************REGLA PARA ASIGNAR CLASES***************************
reglas <- function(x){
if (x < 4.99735){
return(1)
}else if (x < 5.989202){
return(2)
}else if (x < 6.994868){
return(3)
}else if (x < 7.997107){
return(4)
}else if (x < 8.992769){
return(5)
}else if (x < 9.987271){
return(6)
}else if (x < 10.99652){
return(7)
}else if (x < 11.99878){
return(8)
}else if (x < 12.99898){
return(9)
}else if (x < 13.98096){
return(10)
}else{
return(11)
}
}
for (x in 1:nrow(df)) {
df$class[x] <- reglas(df$class[x])
}
